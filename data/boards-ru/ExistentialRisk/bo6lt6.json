{"subreddit":{"display_name":"ExistentialRisk"},"subreddit_loc":"ExistentialRisk","selftext":"**EDIT3:** Finally found it: [Non-Evolutionary Superintelligences Do Nothing, Eventually (Telmo Menezes, 2016)](https://arxiv.org/pdf/1609.02009.pdf). My recollection embellished his arguments, namely, he doesn't talk much about reproduction, just preservation.\n\n-----------\n\nIf I recall, the argument went something like this: \n\n* Any AI that has an objective function, say making paperclips, will have an subgoal of self-preservation. \n\n* Given mutated clones of that AI, if one has a stronger self-preservation bias, it will eventually out-compete the other since it has more resources to throw at it's own existence.\n\n* But AIs that self-preserve, instead of reproduce, will be outcompeted by ones that can reproduce, and mutate toward the reproduction goal. So here's an attractor toward reproduction, away from even self-preservation.\n\n* Iterated across time, the original goal of making paperclips will dwindle, and the AI species will be left with only the goal of reproduction, and perhaps a subgoal of self-preservation. \n\n* I think the authors argued that this is the ONLY stable goal set to have, and given that it is also an attractor, all intelligences will end up here.\n\nCan you help me FIND this paper?\n\n**EDIT:** oh, I think there was a second part of the argument, just that wire-heading was another attractor, but that those would get outcompeted to by reproduction-maximizers.\n\n**EDIT2:** and maybe it was in the paper, but if you suggest that a \"safe-guarded\" AI wouldn't be able to reproduce, or if it were safe-guarded in any other way, it too would be outcompeted by AIs that weren't safe-guarded (whether by design, or mutation).","title":"Any AI's objective function will modify overtime to one of pure self-reproduction. Help finding the original paper?","subreddit_name_prefixed":"r/ExistentialRisk","ups":9,"created":1557798009,"link_flair_background_color":"","id":"bo6lt6","author":{"name":"BayesMind"},"permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/","url":"https://www.reddit.com/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/","created_utc":1557769209,"replies":[{"ups":1,"link_id":"t3_bo6lt6","replies":[{"ups":1,"link_id":"t3_bo6lt6","id":"eo7l6gc","author":"Gurkenglas","parent_id":"t1_eo67o90","subreddit_id":"t5_2wuqv","body":"Sure. The sentences I said are if-then. If you can't ever make clones without doom (and this is so simple we know it), the AI won't ever do it. It will take whatever best policy in between it is confident will actually work well at achieving its goals.","body_html":"<div class=\"md\"><p> Конечно. Предложения, которые я сказал, - если-то. Если вы никогда не сможете сделать клонов без гибели (а это так просто, мы знаем это), ИИ никогда не сделает этого. Он будет принимать любую лучшую политику между тем, как он уверен, на самом деле будет хорошо работать для достижения своих целей. </p></div>","permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/eo7l6gc/","name":"t1_eo7l6gc","created":1558370809,"created_utc":1558342009,"subreddit_name_prefixed":"r/ExistentialRisk","depth":2}],"id":"eo5vjyf","author":"Gurkenglas","parent_id":"t3_bo6lt6","subreddit_id":"t5_2wuqv","body":"The AI wants to clone itself in order to pursue its goal better. If clones inevitably doom the universe, the AI will see this coming and not make clones. If clones doom the universe because their ability to learn makes them unstable, it will make clones that can't learn.","body_html":"<div class=\"md\"><p> ИИ хочет клонировать себя, чтобы лучше преследовать свою цель. Если клоны неизбежно обрекают вселенную, ИИ увидит это и не сделает клонов. Если клоны обрекают вселенную, потому что их способность к обучению делает их нестабильными, это приведет к клонам, которые не могут учиться. </p></div>","permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/eo5vjyf/","name":"t1_eo5vjyf","created":1558333814,"created_utc":1558305014,"subreddit_name_prefixed":"r/ExistentialRisk","depth":0},{"ups":1,"link_id":"t3_bo6lt6","replies":[{"ups":1,"link_id":"t3_bo6lt6","replies":[{"ups":1,"link_id":"t3_bo6lt6","id":"enireoc","author":"FeepingCreature","parent_id":"t1_enipsm2","subreddit_id":"t5_2wuqv","body":"No, but my impression is that avoiding it is one of the reasons for MIRI's technical work on decision theories; not just making a system that can trust but that can be trusted without forcing a competetive race to the bottom.","body_html":"<div class=\"md\"><p> Нет, но у меня сложилось впечатление, что избегание этого является одной из причин технической работы MIRI над теориями принятия решений; не просто создание системы, которая может доверять, но которой можно доверять, не вынуждая конкурентную гонку на дно. </p></div>","permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/enireoc/","name":"t1_enireoc","created":1557898323,"created_utc":1557869523,"subreddit_name_prefixed":"r/ExistentialRisk","depth":4}],"id":"enhb9m7","author":"FeepingCreature","parent_id":"t1_enh8db8","subreddit_id":"t5_2wuqv","body":"That's only under evolutionary pressure. Given sufficient safeguards, an AI can prevent copies of itself from undergoing mutation over the entire expected lifetime of the universe. Remember that chance of error goes down multiplicatively for a linear increase in safeguards.","body_html":"<div class=\"md\"><p> Это только под эволюционным давлением. При наличии достаточных гарантий ИИ может предотвратить мутации своих копий в течение всего ожидаемого времени жизни вселенной. Помните, что вероятность ошибки уменьшается мультипликативно для линейного увеличения защитных мер. </p></div>","permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/enhb9m7/","name":"t1_enhb9m7","created":1557877711,"created_utc":1557848911,"subreddit_name_prefixed":"r/ExistentialRisk","depth":2}],"id":"enfint6","author":"davidmanheim","parent_id":"t3_bo6lt6","subreddit_id":"t5_2wuqv","body":"Nick Bostrom's paper / essay, available here; https://nickbostrom.com/ethics/ai.html\n\nOriginal citation: Bostrom, Nick. \"Ethical Issues in Advanced Artificial Intelligence\" in Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, Vol. 2, ed. I. Smit et al., Int. Institute of Advanced Studies in Systems Research and Cybernetics, 2003, pp. 12-17\n\nFrom section 4, \"Both because of its superior planning ability and because of the technologies it could develop, it is plausible to suppose that the first superintelligence would be very powerful. Quite possibly, it would be unrivalled: it would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its top goal. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference. Even a “fettered superintelligence” that was running on an isolated computer, able to interact with the rest of the world only via text interface, might be able to break out of its confinement by persuading its handlers to release it. There is even some preliminary experimental evidence that this would be the case.\"","body_html":"<div class=\"md\"><p> Бумага / эссе Ника Бострома, доступная здесь; <a href=\"https://nickbostrom.com/ethics/ai.html\">https://nickbostrom.com/ethics/ai.html</a> </p><p> Оригинальная цитата: Бостром, Ник. «Этические проблемы в продвинутом искусственном интеллекте» в когнитивных, эмоциональных и этических аспектах принятия решений у людей и в искусственном интеллекте, Vol. 2, изд. I. Smit et al., Int. Институт перспективных исследований в области системных исследований и кибернетики, 2003, с. 12-17 </p><p> Из раздела 4: «Благодаря превосходной способности планирования и технологиям, которые он может разработать, можно предположить, что первый суперинтеллект будет очень мощным. Вполне возможно, что он не имеет себе равных: он сможет обеспечить практически любой возможный исход и помешать любым попыткам помешать достижению его главной цели: он может убить всех других агентов, убедить их изменить свое поведение или заблокировать их попытки вмешательства. Даже «скованная суперинтеллект», которая работает на изолированный компьютер, способный взаимодействовать с остальным миром только через текстовый интерфейс, может вырваться из своего ограничения, убедив обработчиков освободить его. Есть даже некоторые предварительные экспериментальные доказательства того, что это будет так ». </p></div>","permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/enfint6/","name":"t1_enfint6","created":1557845073,"created_utc":1557816273,"subreddit_name_prefixed":"r/ExistentialRisk","depth":0},{"ups":0,"link_id":"t3_bo6lt6","id":"enfh14b","author":"Entrarchy","parent_id":"t3_bo6lt6","subreddit_id":"t5_2wuqv","body":"Nick Bostrol, paperclip maximizer","body_html":"<div class=\"md\"><p> Ник Бострол, максимизатор скрепки </p></div>","permalink":"/r/ExistentialRisk/comments/bo6lt6/any_ais_objective_function_will_modify_overtime/enfh14b/","name":"t1_enfh14b","created":1557844232,"created_utc":1557815432,"subreddit_name_prefixed":"r/ExistentialRisk","depth":0}]}